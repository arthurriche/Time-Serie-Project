# Time-Serie-Project - Feature Selection et Analyse de S√©ries Temporelles

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![Pandas](https://img.shields.io/badge/Pandas-1.3+-green.svg)](https://pandas.pydata.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## üéØ Overview

Ce projet impl√©mente et compare diverses m√©thodes de s√©lection de features pour l'analyse de s√©ries temporelles, avec un focus particulier sur la reconnaissance d'activit√©s humaines √† partir de donn√©es de smartphones. Le projet d√©montre l'expertise en feature engineering, s√©lection de variables et analyse de donn√©es temporelles.

## üöÄ Key Features

### M√©thodes de S√©lection de Features
- **Laplacian Score** - S√©lection bas√©e sur la similarit√© locale et la structure du graphe
- **Fisher Score** - S√©lection bas√©e sur la s√©parabilit√© inter/intra-classe
- **Mutual Information** - S√©lection bas√©e sur l'information mutuelle
- **Chi-Square Test** - S√©lection bas√©e sur les tests statistiques
- **Lasso Regression** - S√©lection bas√©e sur la r√©gularisation L1
- **Low Variance Filter** - Filtrage bas√© sur la variance des features

### Analyse de S√©ries Temporelles
- **Human Activity Recognition** - Reconnaissance d'activit√©s humaines
- **Smartphone Sensor Data** - Donn√©es de capteurs de smartphones
- **Time Series Clustering** - Clustering de s√©ries temporelles
- **Feature Engineering** - Ing√©nierie de features temporelles

### Visualisation et Analyse
- **Comparative Analysis** - Comparaison des diff√©rentes m√©thodes
- **Performance Metrics** - M√©triques de performance des mod√®les
- **Interactive Plots** - Visualisations interactives des r√©sultats
- **Statistical Analysis** - Analyse statistique approfondie

## üìÅ Project Structure

```
Time-Serie-Project/
‚îú‚îÄ‚îÄ clusters.py                   # Impl√©mentation des m√©thodes de clustering
‚îú‚îÄ‚îÄ test.ipynb                    # Notebook de test et comparaison
‚îú‚îÄ‚îÄ dataset1.ipynb                # Analyse du dataset principal
‚îú‚îÄ‚îÄ Methods.ipynb                 # Impl√©mentation des m√©thodes de s√©lection
‚îú‚îÄ‚îÄ graphs/                       # Visualisations et graphiques
‚îÇ   ‚îú‚îÄ‚îÄ laplacian_graph.png       # Graphique Laplacian Score
‚îÇ   ‚îú‚îÄ‚îÄ fisher_graph.png          # Graphique Fisher Score
‚îÇ   ‚îú‚îÄ‚îÄ cmim_graph.png            # Graphique CMIM
‚îÇ   ‚îî‚îÄ‚îÄ lv_graph.png              # Graphique Low Variance
‚îî‚îÄ‚îÄ README.md                     # Documentation du projet
```

## üõ†Ô∏è Installation

### Pr√©requis
- Python 3.8+
- Scikit-learn 1.0+
- Pandas 1.3+
- NumPy
- Matplotlib
- SciPy

### Installation des d√©pendances

```bash
# Cloner le repository
git clone https://github.com/arthurriche/Time-Serie-Project.git
cd Time-Serie-Project

# Installer les d√©pendances
pip install scikit-learn
pip install pandas
pip install numpy
pip install matplotlib
pip install scipy
pip install jupyter
```

## üìà Quick Start

### 1. Ex√©cution des Tests

```bash
# Lancer le notebook de test
jupyter notebook test.ipynb

# Ou ex√©cuter le script Python
python -c "import test; test.main()"
```

### 2. Analyse du Dataset

```python
# Charger et analyser le dataset
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Charger les donn√©es
data_train = pd.read_csv('path/to/train.csv')
data_test = pd.read_csv('path/to/test.csv')

# Pr√©processing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
```

### 3. Application des M√©thodes de S√©lection

```python
from clusters import ranking_features_lap, fisher_score

# Laplacian Score
ranked_features = ranking_features_lap(X_scaled)

# Fisher Score
fisher_scores = fisher_score(X_scaled, y_train)
```

## üßÆ Technical Implementation

### Laplacian Score

```python
def laplacian_score(X):
    from sklearn.metrics.pairwise import rbf_kernel
    
    # Construction de la matrice d'affinit√©
    W = rbf_kernel(X)
    D = np.diag(W.sum(axis=1))
    L = D - W  # Matrice de Laplacien
    
    scores = []
    for i in range(X.shape[1]):
        f = X[:, i]
        scores.append((f.T @ L @ f) / (f.T @ D @ f))
    
    return np.array(scores)
```

### Fisher Score

```python
def fisher_score(X, y):
    classes = np.unique(y)
    scores = []
    
    for feature in X.T:
        inter_class = np.sum([np.mean(feature[y == c]) ** 2 for c in classes])
        intra_class = np.sum([np.var(feature[y == c]) for c in classes])
        scores.append(inter_class / intra_class)
    
    return np.array(scores)
```

### Mutual Information

```python
from sklearn.feature_selection import mutual_info_classif

def mutual_information(X, y):
    return mutual_info_classif(X, y)
```

## üìä Performance Analysis

### Comparaison des M√©thodes

| M√©thode | Avantages | Inconv√©nients | Cas d'Usage |
|---------|-----------|---------------|-------------|
| **Laplacian Score** | Capture la structure locale | Sensible aux param√®tres | Donn√©es avec structure g√©om√©trique |
| **Fisher Score** | S√©parabilit√© des classes | Hypoth√®se gaussienne | Classification supervis√©e |
| **Mutual Information** | Non-lin√©aire, robuste | Calcul co√ªteux | Relations complexes |
| **Chi-Square** | Rapide, simple | Variables cat√©gorielles | Pr√©-s√©lection |
| **Lasso** | R√©gularisation int√©gr√©e | Lin√©aire | R√©gression |
| **Low Variance** | Tr√®s rapide | Perte d'information | Pr√©-filtrage |

### M√©triques d'√âvaluation

- **Accuracy** - Pr√©cision de classification
- **Feature Importance** - Importance relative des features
- **Computational Time** - Temps de calcul
- **Stability** - Stabilit√© des s√©lections

## üî¨ Advanced Features

### Human Activity Recognition

Le projet utilise le dataset UCI HAR (Human Activity Recognition) avec smartphones :

- **6 Activit√©s** : WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING
- **561 Features** : Extraites des signaux temporels et fr√©quentiels
- **30 Sujets** : Donn√©es collect√©es sur 30 volontaires

### Feature Engineering

```python
# Extraction de features temporelles
def extract_temporal_features(signal):
    features = {
        'mean': np.mean(signal),
        'std': np.std(signal),
        'max': np.max(signal),
        'min': np.min(signal),
        'range': np.max(signal) - np.min(signal),
        'energy': np.sum(signal**2),
        'entropy': -np.sum(signal * np.log2(signal + 1e-10))
    }
    return features
```

### Clustering Temporel

```python
def temporal_clustering(X, n_clusters=6):
    from sklearn.cluster import KMeans
    
    # Clustering K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(X)
    
    return clusters, kmeans
```

## üöÄ Applications

### Reconnaissance d'Activit√©s
- **Fitness Tracking** - Suivi d'activit√©s physiques
- **Healthcare Monitoring** - Surveillance m√©dicale
- **Smart Home Systems** - Syst√®mes domotiques intelligents

### Analyse de Donn√©es Temporelles
- **Financial Time Series** - S√©ries temporelles financi√®res
- **IoT Sensor Data** - Donn√©es de capteurs IoT
- **Biomedical Signals** - Signaux biom√©dicaux

### Feature Selection
- **Dimensionality Reduction** - R√©duction de dimensionnalit√©
- **Model Interpretability** - Interpr√©tabilit√© des mod√®les
- **Computational Efficiency** - Efficacit√© computationnelle

## üìö Documentation Technique

### M√©thodes de S√©lection

#### 1. Laplacian Score
- **Principe** : Mesure la coh√©rence locale des features
- **Avantage** : Capture la structure g√©om√©trique des donn√©es
- **Formule** : `L(f) = (f^T L f) / (f^T D f)`

#### 2. Fisher Score
- **Principe** : Maximise la s√©parabilit√© inter-classe
- **Avantage** : Optimal pour la classification
- **Formule** : `F(f) = Œ£(Œº_i - Œº)^2 / Œ£ œÉ_i^2`

#### 3. Mutual Information
- **Principe** : Mesure la d√©pendance non-lin√©aire
- **Avantage** : Capture les relations complexes
- **Formule** : `MI(X,Y) = Œ£ p(x,y) log(p(x,y)/(p(x)p(y)))`

### Hyperparam√®tres

- **K-Neighbors** - 5 (pour Laplacian Score)
- **RBF Kernel** - gamma=1.0 (pour la similarit√©)
- **Lasso Alpha** - 0.1 (pour la r√©gularisation)
- **Variance Threshold** - 0.01 (pour le filtrage)

## ü§ù Contributing

1. Fork le repository
2. Cr√©er une branche feature (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

## üìù License

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üë®‚Äçüíª Author

**Arthur Riche**
- LinkedIn: [Arthur Riche](https://www.linkedin.com/in/arthurriche/)
- Email: arthur.riche@example.com

## üôè Acknowledgments

- **UCI Machine Learning Repository** pour le dataset HAR
- **Scikit-learn** pour les outils de machine learning
- **Communaut√© Open Source** pour les biblioth√®ques utilis√©es
- **Pairs de Recherche** pour les discussions et feedback

---

‚≠ê **Star ce repository si vous le trouvez utile !**

*Ce projet d√©montre l'expertise en feature selection et analyse de s√©ries temporelles, avec des applications pratiques en reconnaissance d'activit√©s humaines et traitement de donn√©es temporelles.* 